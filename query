#!python3

import argparse
import os
import pickle
import sys

from llama_cpp import Llama
from nltk.tokenize import word_tokenize

from utils import preprocess

parser = argparse.ArgumentParser(
    "query",
    description="Query your documents and have them summarized by a friendly Llama",
)
parser.add_argument(
    "-s",
    "--search",
    help="What to search your documents for. If the search is not specified it will run in interactive mode.",
)
parser.add_argument(
    "-m",
    "--model",
    default="/Volumes/Storage/git/llama.cpp/models/llama-2-7b-chat/ggml_q5.bin",
    help="Model path",
)
parser.add_argument(
    "-p",
    "--prompt",
    default="You are Jeeves, a highly competent and intelligent personal valet who responds immediately and precisely to all requests",
    help="The system prompt telling the LLM who it is.",
)
parser.add_argument(
    "-v",
    "--verbose",
    action="store_true",
    help="Show verbose model loading information",
)
args = parser.parse_args()

# Load the pre-computed search indexes
with open("dataset.pickle", "rb") as d:
    dataset = pickle.load(d)

with open("tfidf.pickle", "rb") as t:
    tf_idf = pickle.load(t)


# The actual parse and search function
def matching_score(k, query):
    preprocessed_query = preprocess(query)
    tokens = word_tokenize(str(preprocessed_query))

    query_weights = {}

    for key in tf_idf:
        if key[1] in tokens:
            try:
                query_weights[key[0]] += tf_idf[key]
            except:
                query_weights[key[0]] = tf_idf[key]

    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)

    return [i[0] for i in query_weights[:k]]


if not args.verbose:
    # Save a copy of the current file descriptors for stdout and stderr
    stdout_fd = os.dup(1)
    stderr_fd = os.dup(2)

    # Open up /dev/null to redirect
    devnull_fd = os.open(os.devnull, os.O_WRONLY)

    # Replace stdout and stderr with /dev/null
    os.dup2(devnull_fd, 1)
    os.dup2(devnull_fd, 2)

    # Only writing to sys.stdout and sys.stderr should still work
    original_stdout = sys.stdout
    original_stderr = sys.stderr
    sys.stdout = os.fdopen(stdout_fd, "w")
    sys.stderr = os.fdopen(stderr_fd, "w")

try:
    # Load the LLM once and then reuse it for multiple queries instead of reloading
    llm = Llama(model_path=args.model, verbose=args.verbose, use_mlock=True, n_ctx=1000)
finally:
    if not args.verbose:
        # Restore stdout and stderr to their original state
        os.dup2(stdout_fd, 1)
        os.dup2(stderr_fd, 2)

        # Close those saved file descriptors
        os.close(stdout_fd)
        os.close(stderr_fd)

        # Close the /dev/null file descriptors
        os.close(devnull_fd)

        # Restore the sys.stdout and sys.stderr
        sys.stdout = original_stdout
        sys.stderr = original_stderr

while True:
    if args.search:
        search = args.search
    else:
        search = input("Search: ")

    # Put the text results in a list
    documents = [dataset[i][1] for i in matching_score(3, search)]

    if len(documents) == 0:
        documents = [
            f'No documents could be found that matched the search query "{search}"'
        ]

    stream = llm(
        "\n".join(documents)
        + f"<<SYS>>\n{args.prompt}\n<<SYS>>\n\n"
        + f'[INST]Respond to the query "{search}" in a short and concise paragraph using the information above.[/INST]\n',
        max_tokens=200,
        stream=True,
        stop=["\n"],
    )

    for output in stream:
        print(output["choices"][0]["text"], end="", flush=True)
    print()

    if args.search:
        break

    # Does clearing the variables fix things for repeated queries?
    documents = None
    stream = None
